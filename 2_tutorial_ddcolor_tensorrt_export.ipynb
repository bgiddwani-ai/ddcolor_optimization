{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDColor ONNX to TensorRT Conversion\n",
    "\n",
    "Simple notebook to convert DDColor ONNX model to TensorRT engine for faster GPU inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model: ./exported/model.onnx\n",
      "Output Engine: ./exported/model.plan\n",
      "Batch sizes: min=1, opt=8, max=16\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "ONNX_MODEL_PATH = \"./exported/model.onnx\"\n",
    "ENGINE_OUTPUT_PATH = \"./exported/model.plan\"\n",
    "\n",
    "# Model settings\n",
    "IMAGE_SIZE = 512  # Width and height\n",
    "MIN_BATCH = 1\n",
    "OPT_BATCH = 8   # Most common batch size\n",
    "MAX_BATCH = 16\n",
    "\n",
    "print(f\"ONNX Model: {ONNX_MODEL_PATH}\")\n",
    "print(f\"Output Engine: {ENGINE_OUTPUT_PATH}\")\n",
    "print(f\"Batch sizes: min={MIN_BATCH}, opt={OPT_BATCH}, max={MAX_BATCH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build TensorRT Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT Conversion Command:\n",
      "==================================================\n",
      "trtexec \\\n",
      "  --onnx=./exported/model.onnx \\\n",
      "  --saveEngine=./exported/model.plan \\\n",
      "  --fp16 \\\n",
      "  --minShapes=input:1x3x512x512 \\\n",
      "  --optShapes=input:8x3x512x512 \\\n",
      "  --maxShapes=input:16x3x512x512\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Build trtexec command\n",
    "trtexec_cmd = f\"\"\"\n",
    "trtexec \\\\\n",
    "  --onnx={ONNX_MODEL_PATH} \\\\\n",
    "  --saveEngine={ENGINE_OUTPUT_PATH} \\\\\n",
    "  --fp16 \\\\\n",
    "  --minShapes=input:{MIN_BATCH}x3x{IMAGE_SIZE}x{IMAGE_SIZE} \\\\\n",
    "  --optShapes=input:{OPT_BATCH}x3x{IMAGE_SIZE}x{IMAGE_SIZE} \\\\\n",
    "  --maxShapes=input:{MAX_BATCH}x3x{IMAGE_SIZE}x{IMAGE_SIZE}\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"TensorRT Conversion Command:\")\n",
    "print(\"=\" * 50)\n",
    "print(trtexec_cmd)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TensorRT conversion...\n",
      "This may take 2-5 minutes depending on your GPU.\n",
      "\n",
      "‚úÖ Conversion successful! (took 197.2 seconds)\n",
      "üìÅ Engine saved to: ./exported/model.plan\n",
      "üìè File size: 113.04 MB\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Check if ONNX file exists\n",
    "if not os.path.exists(ONNX_MODEL_PATH):\n",
    "    print(f\"‚ö†Ô∏è  ONNX file not found at {ONNX_MODEL_PATH}\")\n",
    "    print(\"Please run the ONNX export notebook first.\")\n",
    "else:\n",
    "    print(\"Starting TensorRT conversion...\")\n",
    "    print(\"This may take 2-5 minutes depending on your GPU.\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run the conversion\n",
    "    result = subprocess.run(\n",
    "        trtexec_cmd.replace('\\\\', '').split(),\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ Conversion successful! (took {elapsed:.1f} seconds)\")\n",
    "        \n",
    "        # Check file size\n",
    "        if os.path.exists(ENGINE_OUTPUT_PATH):\n",
    "            size_mb = os.path.getsize(ENGINE_OUTPUT_PATH) / (1024 * 1024)\n",
    "            print(f\"üìÅ Engine saved to: {ENGINE_OUTPUT_PATH}\")\n",
    "            print(f\"üìè File size: {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"‚ùå Conversion failed!\")\n",
    "        print(\"Error output:\")\n",
    "        print(result.stderr[-1000:])  # Last 1000 chars of error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Your DDColor model has been converted to TensorRT format!\n",
    "\n",
    "**Key points:**\n",
    "- ‚úÖ FP16 precision enabled for 2x speedup\n",
    "- ‚úÖ Dynamic batching from 1 to 16\n",
    "- ‚úÖ Optimized for 512x512 input images\n",
    "- ‚úÖ Ready for production deployment\n",
    "\n",
    "The `.plan` file can now be used for fast GPU inference in your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
