{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDColor ONNX Export - Dynamic Batch Size\n",
    "\n",
    "Export DDColor model to ONNX with dynamic batch size support (fixed 512x512 spatial dimensions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if packages are not installed\n",
    "# !pip install onnxsim onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0a0+df5bbc09d1.nv24.12\n",
      "ONNX version: 1.16.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import onnxsim\n",
    "from onnx import load_model, save_model, shape_inference\n",
    "from onnxruntime.tools.symbolic_shape_infer import SymbolicShapeInference\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('/workspace/DDColor')\n",
    "from basicsr.archs.ddcolor_arch import DDColor\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX version: {onnx.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: ./DDColor/pretrain/ddcolor_paper_tiny.pth\n",
      "  Size: tiny\n",
      "  Decoder: MultiScaleColorDecoder\n",
      "  Output: ./exported/model.onnx\n",
      "  Spatial dims: 512x512 (fixed)\n",
      "  Batch size: dynamic\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_PATH = \"./DDColor/pretrain/ddcolor_paper_tiny.pth\"  # Path to your trained DDColor model\n",
    "MODEL_SIZE = \"tiny\"  # Options: \"tiny\" or \"large\"\n",
    "DECODER_TYPE = \"MultiScaleColorDecoder\"  # Options: \"MultiScaleColorDecoder\" or \"SingleColorDecoder\"\n",
    "\n",
    "# Export settings\n",
    "EXPORT_PATH = \"./exported/model.onnx\"  # Output ONNX file\n",
    "OPSET_VERSION = 12  # ONNX opset version\n",
    "\n",
    "# Fixed spatial dimensions (DO NOT CHANGE)\n",
    "INPUT_SIZE = [512, 512]\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  Size: {MODEL_SIZE}\")\n",
    "print(f\"  Decoder: {DECODER_TYPE}\")\n",
    "print(f\"  Output: {EXPORT_PATH}\")\n",
    "print(f\"  Spatial dims: {INPUT_SIZE[0]}x{INPUT_SIZE[1]} (fixed)\")\n",
    "print(f\"  Batch size: dynamic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create and Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model created: convnext-t encoder, MultiScaleColorDecoder decoder\n",
      "‚úÖ Weights loaded from ./DDColor/pretrain/ddcolor_paper_tiny.pth\n",
      "\n",
      "üîÑ Exporting to ONNX with dynamic batch size...\n",
      "‚úÖ ONNX export complete: ./exported/model.onnx\n"
     ]
    }
   ],
   "source": [
    "def create_and_export_model():\n",
    "    device = torch.device(\"cpu\")\n",
    "    encoder_name = \"convnext-t\" if MODEL_SIZE == \"tiny\" else \"convnext-l\"\n",
    "    \n",
    "    # Create model with fixed 512x512 input size\n",
    "    model = DDColor(\n",
    "        encoder_name=encoder_name,\n",
    "        decoder_name=DECODER_TYPE,\n",
    "        input_size=INPUT_SIZE,\n",
    "        num_output_channels=2,\n",
    "        last_norm=\"Spectral\",\n",
    "        do_normalize=False,\n",
    "        num_queries=100,\n",
    "        num_scales=3,\n",
    "        dec_layers=9,\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"‚úÖ Model created: {encoder_name} encoder, {DECODER_TYPE} decoder\")\n",
    "    \n",
    "    # Load pretrained weights\n",
    "    try:\n",
    "        ckpt = torch.load(MODEL_PATH, map_location=device)\n",
    "        model.load_state_dict(ckpt[\"params\"], strict=False)\n",
    "        print(f\"‚úÖ Weights loaded from {MODEL_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è  No weights found at {MODEL_PATH}, using random initialization\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input: [batch=1, channels=3, height=512, width=512]\n",
    "    dummy_input = torch.randn((1, 3, INPUT_SIZE[0], INPUT_SIZE[1]), dtype=torch.float32)\n",
    "    \n",
    "    # ONLY batch dimension is dynamic, spatial dimensions are fixed at 512x512\n",
    "    dynamic_axes = {\n",
    "        \"input\": {0: \"batch\"},   # Only batch is dynamic\n",
    "        \"output\": {0: \"batch\"},  # Only batch is dynamic\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüîÑ Exporting to ONNX with dynamic batch size...\")\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        EXPORT_PATH,\n",
    "        export_params=True,\n",
    "        opset_version=OPSET_VERSION,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        do_constant_folding=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ ONNX export complete: {EXPORT_PATH}\")\n",
    "\n",
    "# Run the export\n",
    "create_and_export_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimize and Verify ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Running ONNX optimization...\n",
      "\n",
      "1. Shape inference...\n",
      "2. Symbolic shape inference...\n",
      "3. Simplifying model...\n",
      "4. Verifying model...\n",
      "\n",
      "‚úÖ Model optimized and verified: ./exported/model.onnx\n",
      "\n",
      "üìä Model Input/Output Info:\n",
      "  Input: input\n",
      "    Shape: ['batch', 3, 512, 512]\n",
      "    Expected: ['batch', 3, 512, 512]\n",
      "\n",
      "  Output: output\n",
      "    Shape: ['batch', 2, 512, 512]\n",
      "    Expected: ['batch', 2, 512, 512]\n",
      "\n",
      "üìÅ File size: 210.31 MB\n"
     ]
    }
   ],
   "source": [
    "def optimize_and_verify_onnx():\n",
    "    print(\"üîß Running ONNX optimization...\\n\")\n",
    "    \n",
    "    # Step 1: Shape inference\n",
    "    print(\"1. Shape inference...\")\n",
    "    model = load_model(EXPORT_PATH)\n",
    "    model = shape_inference.infer_shapes(model)\n",
    "    save_model(model, EXPORT_PATH)\n",
    "    \n",
    "    # Step 2: Symbolic shape inference (better dynamic handling)\n",
    "    print(\"2. Symbolic shape inference...\")\n",
    "    model = SymbolicShapeInference.infer_shapes(\n",
    "        load_model(EXPORT_PATH),\n",
    "        auto_merge=True,\n",
    "        guess_output_rank=True,\n",
    "    )\n",
    "    save_model(model, EXPORT_PATH)\n",
    "    \n",
    "    # Step 3: Simplify\n",
    "    print(\"3. Simplifying model...\")\n",
    "    model_simplified, check = onnxsim.simplify(model)\n",
    "    assert check, \"ONNX simplification failed!\"\n",
    "    onnx.save(model_simplified, EXPORT_PATH)\n",
    "    \n",
    "    # Step 4: Verify\n",
    "    print(\"4. Verifying model...\")\n",
    "    onnx_model = onnx.load(EXPORT_PATH)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Model optimized and verified: {EXPORT_PATH}\")\n",
    "    \n",
    "    # Display model info\n",
    "    print(\"\\nüìä Model Input/Output Info:\")\n",
    "    for input_tensor in onnx_model.graph.input:\n",
    "        print(f\"  Input: {input_tensor.name}\")\n",
    "        shape = [dim.dim_param if dim.dim_param else dim.dim_value \n",
    "                for dim in input_tensor.type.tensor_type.shape.dim]\n",
    "        print(f\"    Shape: {shape}\")\n",
    "        print(f\"    Expected: ['batch', 3, 512, 512]\")\n",
    "    \n",
    "    for output_tensor in onnx_model.graph.output:\n",
    "        print(f\"\\n  Output: {output_tensor.name}\")\n",
    "        shape = [dim.dim_param if dim.dim_param else dim.dim_value \n",
    "                for dim in output_tensor.type.tensor_type.shape.dim]\n",
    "        print(f\"    Shape: {shape}\")\n",
    "        print(f\"    Expected: ['batch', 2, 512, 512]\")\n",
    "    \n",
    "    # File size\n",
    "    import os\n",
    "    file_size_mb = os.path.getsize(EXPORT_PATH) / (1024 * 1024)\n",
    "    print(f\"\\nüìÅ File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "# Optimize and verify\n",
    "optimize_and_verify_onnx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ONNX model with different batch sizes:\n",
      "\n",
      "Batch size  1: Input (1, 3, 512, 512) ‚Üí Output (1, 2, 512, 512) ‚úÖ\n",
      "Batch size  4: Input (4, 3, 512, 512) ‚Üí Output (4, 2, 512, 512) ‚úÖ\n",
      "Batch size  8: Input (8, 3, 512, 512) ‚Üí Output (8, 2, 512, 512) ‚úÖ\n",
      "Batch size 16: Input (16, 3, 512, 512) ‚Üí Output (16, 2, 512, 512) ‚úÖ\n",
      "\n",
      "‚úÖ Dynamic batching works correctly!\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "def test_onnx_model():\n",
    "    \"\"\"Test the ONNX model with different batch sizes.\"\"\"\n",
    "\n",
    "    providers = [\n",
    "        # The TensorrtExecutionProvider is the fastest.\n",
    "        ('TensorrtExecutionProvider', { \n",
    "            'device_id': 0,\n",
    "            'trt_max_workspace_size': 4 * 1024 * 1024 * 1024,\n",
    "            'trt_fp16_enable': True,\n",
    "            'trt_engine_cache_enable': True,\n",
    "            'trt_engine_cache_path': './trt_engine_cache',\n",
    "            'trt_engine_cache_prefix': 'model',\n",
    "            'trt_dump_subgraphs': False,\n",
    "            'trt_timing_cache_enable': True,\n",
    "            'trt_timing_cache_path': './trt_engine_cache',\n",
    "            #'trt_builder_optimization_level': 3,\n",
    "        })]\n",
    "    # Create ONNX session\n",
    "    session = ort.InferenceSession(EXPORT_PATH, providers=providers)\n",
    "    \n",
    "    print(\"Testing ONNX model with different batch sizes:\\n\")\n",
    "    \n",
    "    # Test different batch sizes\n",
    "    for batch_size in [1, 4, 8, 16]:\n",
    "        # Create test input\n",
    "        test_input = np.random.randn(batch_size, 3, 512, 512).astype(np.float32)\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = session.run(None, {\"input\": test_input})\n",
    "        \n",
    "        print(f\"Batch size {batch_size:2d}: Input {test_input.shape} ‚Üí Output {outputs[0].shape} ‚úÖ\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Dynamic batching works correctly!\")\n",
    "\n",
    "# Test the model\n",
    "try:\n",
    "    test_onnx_model()\n",
    "except Exception as e:\n",
    "    print(f\"Could not test model: {e}\")\n",
    "    print(\"This is normal if onnxruntime is not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Your DDColor model has been exported to ONNX format!\n",
    "\n",
    "**Key features:**\n",
    "- ‚úÖ Dynamic batch size support\n",
    "- ‚úÖ Fixed 512x512 spatial dimensions\n",
    "- ‚úÖ Optimized and simplified\n",
    "- ‚úÖ Ready for TensorRT conversion\n",
    "\n",
    "**Next step:** Use the TensorRT notebook to convert this ONNX model to TensorRT engine for faster GPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
