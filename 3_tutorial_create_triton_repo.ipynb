{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Model Configuration Generator\n",
    "\n",
    "This notebook creates `config.pbtxt` files for DDColor models in TensorRT and ONNX formats for Triton Inference Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the base workspace path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT model path: /workspace/model_repository_trt/ddcolor_trt\n",
      "ONNX model path: /workspace/model_repository_onnx/ddcolor_onnx\n"
     ]
    }
   ],
   "source": [
    "# Base workspace path\n",
    "workspace_path_trt = Path('/workspace/model_repository_trt')\n",
    "workspace_path_onnx = Path('/workspace/model_repository_onnx')\n",
    "\n",
    "workspace_path_trt.mkdir(parents=True, exist_ok=True)\n",
    "workspace_path_onnx.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "trt_model_path = workspace_path_trt / 'ddcolor_trt'\n",
    "onnx_model_path = workspace_path_onnx / 'ddcolor_onnx'\n",
    "\n",
    "trt_model_path.mkdir(parents=True, exist_ok=True)\n",
    "onnx_model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"TensorRT model path: {trt_model_path}\")\n",
    "print(f\"ONNX model path: {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create config.pbtxt for TensorRT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT config written to: /workspace/model_repository_trt/ddcolor_trt/config.pbtxt\n",
      "\n",
      "TensorRT Configuration:\n",
      "--------------------------------------------------\n",
      "name: \"ddcolor_trt\"\n",
      "platform: \"tensorrt_plan\"\n",
      "max_batch_size: 16\n",
      "input [\n",
      "  {\n",
      "    name: \"input\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [ 3, 512, 512 ]\n",
      "  }\n",
      "]\n",
      "output [\n",
      "  {\n",
      "    name: \"output\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [ 2, 512, 512 ]\n",
      "  }\n",
      "]\n",
      "instance_group [\n",
      "  {\n",
      "    count: 1\n",
      "    kind: KIND_GPU\n",
      "    gpus: [ 0 ]\n",
      "  }\n",
      "]\n",
      "dynamic_batching {\n",
      "  preferred_batch_size: [ 1, 2, 4, 8, 12, 16 ]\n",
      "  max_queue_delay_microseconds: 100\n",
      "}\n",
      "optimization {\n",
      "  cuda {\n",
      "    graphs: true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# TensorRT configuration\n",
    "trt_config = \"\"\"name: \"ddcolor_trt\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 16\n",
    "input [\n",
    "  {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 3, 512, 512 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 2, 512, 512 ]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "    gpus: [ 0 ]\n",
    "  }\n",
    "]\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 1, 2, 4, 8, 12, 16 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "optimization {\n",
    "  cuda {\n",
    "    graphs: true\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "# Write TensorRT config\n",
    "trt_config_path = trt_model_path / 'config.pbtxt'\n",
    "with open(trt_config_path, 'w') as f:\n",
    "    f.write(trt_config)\n",
    "\n",
    "print(f\"TensorRT config written to: {trt_config_path}\")\n",
    "print(\"\\nTensorRT Configuration:\")\n",
    "print(\"-\" * 50)\n",
    "print(trt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create config.pbtxt for ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX config written to: /workspace/model_repository_onnx/ddcolor_onnx/config.pbtxt\n",
      "\n",
      "ONNX Configuration:\n",
      "--------------------------------------------------\n",
      "name: \"ddcolor_onnx\"\n",
      "platform: \"onnxruntime_onnx\"\n",
      "max_batch_size: 16\n",
      "input [\n",
      "  {\n",
      "    name: \"input\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [ 3, 512, 512 ]\n",
      "  }\n",
      "]\n",
      "output [\n",
      "  {\n",
      "    name: \"output\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [ 2, 512, 512 ]\n",
      "  }\n",
      "]\n",
      "instance_group [\n",
      "  {\n",
      "    count: 1\n",
      "    kind: KIND_GPU\n",
      "    gpus: [ 0 ]\n",
      "  }\n",
      "]\n",
      "dynamic_batching {\n",
      "  preferred_batch_size: [ 1, 2, 4, 8, 12, 16 ]\n",
      "  max_queue_delay_microseconds: 100\n",
      "}\n",
      "optimization {\n",
      "  execution_accelerators {\n",
      "    gpu_execution_accelerator [\n",
      "      {\n",
      "        name: \"tensorrt\"\n",
      "        parameters { key: \"precision_mode\" value: \"FP16\" }\n",
      "        parameters { key: \"max_workspace_size_bytes\" value: \"1073741824\" }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ONNX configuration\n",
    "onnx_config = \"\"\"name: \"ddcolor_onnx\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 16\n",
    "input [\n",
    "  {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 3, 512, 512 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 2, 512, 512 ]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "    gpus: [ 0 ]\n",
    "  }\n",
    "]\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 1, 2, 4, 8, 12, 16 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "optimization {\n",
    "  execution_accelerators {\n",
    "    gpu_execution_accelerator [\n",
    "      {\n",
    "        name: \"tensorrt\"\n",
    "        parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "        parameters { key: \"max_workspace_size_bytes\" value: \"1073741824\" }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "# Write ONNX config\n",
    "onnx_config_path = onnx_model_path / 'config.pbtxt'\n",
    "with open(onnx_config_path, 'w') as f:\n",
    "    f.write(onnx_config)\n",
    "\n",
    "print(f\"ONNX config written to: {onnx_config_path}\")\n",
    "print(\"\\nONNX Configuration:\")\n",
    "print(\"-\" * 50)\n",
    "print(onnx_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the created files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying created files:\n",
      "==================================================\n",
      "✅ TensorRT config exists at: /workspace/model_repository_trt/ddcolor_trt/config.pbtxt\n",
      "   File size: 466 bytes\n",
      "✅ ONNX config exists at: /workspace/model_repository_onnx/ddcolor_onnx/config.pbtxt\n",
      "   File size: 684 bytes\n"
     ]
    }
   ],
   "source": [
    "# Verify files were created\n",
    "print(\"\\nVerifying created files:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check TensorRT config\n",
    "if trt_config_path.exists():\n",
    "    print(f\"✅ TensorRT config exists at: {trt_config_path}\")\n",
    "    print(f\"   File size: {trt_config_path.stat().st_size} bytes\")\n",
    "else:\n",
    "    print(f\"❌ TensorRT config not found at: {trt_config_path}\")\n",
    "\n",
    "# Check ONNX config\n",
    "if onnx_config_path.exists():\n",
    "    print(f\"✅ ONNX config exists at: {onnx_config_path}\")\n",
    "    print(f\"   File size: {onnx_config_path.stat().st_size} bytes\")\n",
    "else:\n",
    "    print(f\"❌ ONNX config not found at: {onnx_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create version directories\n",
    "\n",
    "Triton typically expects model files to be in version directories. Run this cell if you want to create version directories for your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created TensorRT version directory: /workspace/model_repository_trt/ddcolor_trt/1\n",
      "Created ONNX version directory: /workspace/model_repository_onnx/ddcolor_onnx/1\n",
      "\n",
      "Note: Place your model files in these version directories:\n",
      "  - TensorRT model (.plan file) → /workspace/model_repository_trt/ddcolor_trt/1/\n",
      "  - ONNX model (.onnx file) → /workspace/model_repository_onnx/ddcolor_onnx/1/\n"
     ]
    }
   ],
   "source": [
    "# Create version directories (optional but recommended for Triton)\n",
    "version = \"1\"\n",
    "\n",
    "# Create version directory for TensorRT model\n",
    "trt_version_path = trt_model_path / version\n",
    "trt_version_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created TensorRT version directory: {trt_version_path}\")\n",
    "\n",
    "# Create version directory for ONNX model\n",
    "onnx_version_path = onnx_model_path / version\n",
    "onnx_version_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created ONNX version directory: {onnx_version_path}\")\n",
    "\n",
    "print(\"\\nNote: Place your model files in these version directories:\")\n",
    "print(f\"  - TensorRT model (.plan file) → {trt_version_path}/\")\n",
    "print(f\"  - ONNX model (.onnx file) → {onnx_version_path}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create version directories\n",
    "\n",
    "Copy model files to the respective directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /workspace/exported/model.plan /workspace/model_repository_trt/ddcolor_trt/1/\n",
    "!cp /workspace/exported/model.onnx /workspace/model_repository_onnx/ddcolor_onnx/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has created the following configuration files:\n",
    "\n",
    "1. **TensorRT Configuration** (`/workspace/model_repository_trt/ddcolor_trt/config.pbtxt`):\n",
    "   - Platform: `tensorrt_plan`\n",
    "   - Max batch size: 16\n",
    "   - Input: FP16, dims [3, 512, 512]\n",
    "   - Output: FP16, dims [2, 512, 512]\n",
    "   - GPU execution with CUDA graphs optimization\n",
    "\n",
    "2. **ONNX Configuration** (`/workspace/model_repository_onnx/ddcolor_onnx/config.pbtxt`):\n",
    "   - Platform: `onnxruntime_onnx`\n",
    "   - Max batch size: 16\n",
    "   - Input: FP32, dims [3, 512, 512]\n",
    "   - Output: FP32, dims [2, 512, 512]\n",
    "   - GPU execution with TensorRT acceleration (FP16 precision)\n",
    "\n",
    "Both models are configured with:\n",
    "- Dynamic batching with preferred sizes: [1, 2, 4, 8, 12, 16]\n",
    "- Single GPU instance (GPU 0)\n",
    "- Max queue delay: 100 microseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
